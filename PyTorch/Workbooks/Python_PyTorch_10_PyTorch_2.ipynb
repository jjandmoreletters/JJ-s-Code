{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch 2 quick Intro\n",
        "\n",
        "* Reference notebook:-https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/pytorch_2_intro.ipynb\n",
        "\n",
        "* Reference book chapter ->https://www.learnpytorch.io/pytorch_2_intro/\n",
        "\n",
        "* PyTorch 2.0 Release notes:-https://pytorch.org/blog/pytorch-2.0-release/"
      ],
      "metadata": {
        "id": "64hVNHnaQPoe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.__version__)"
      ],
      "metadata": {
        "id": "5ZxwYD2AQ7qO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Before PyTorch 2.0"
      ],
      "metadata": {
        "id": "IPu7F5niQ7oI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n",
        "model = torchvision.models.resnet50() #note this could be any model"
      ],
      "metadata": {
        "id": "jm13SsWsQ7lV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### After PyTorch 2.0"
      ],
      "metadata": {
        "id": "sKgmpnubQ7ib"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: some PyTorch 2.0 features may hinder the deployment of models:- https://pytorch.org/get-started/pytorch-2.0/#inference-and-export"
      ],
      "metadata": {
        "id": "roHHHq76SZTW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n",
        "model = torchvision.models.resnet50() #note this could be any model\n",
        "compiled_model = torch.compile(model)\n",
        "\n",
        "##training code\n",
        "\n",
        "##testing code"
      ],
      "metadata": {
        "id": "3MIimi3dQ7fN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Getting Setup"
      ],
      "metadata": {
        "id": "QA5LyzSpS3E6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Check PyTorch version\n",
        "pt_version = torch.__version__\n",
        "print(f\"[INFO] Current PyTorch version: {pt_version} (should be 2.x+)\")\n",
        "\n",
        "# Install PyTorch 2.0 if necessary\n",
        "if pt_version.split(\".\")[0] == \"1\": # Check if PyTorch version begins with 1\n",
        "    !pip3 install -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "    print(\"[INFO] PyTorch 2.x installed, if you're on Google Colab, you may need to restart your runtime.\\\n",
        "          Though as of April 2023, Google Colab comes with PyTorch 2.0 pre-installed.\")\n",
        "    import torch\n",
        "    pt_version = torch.__version__\n",
        "    print(f\"[INFO] Current PyTorch version: {pt_version} (should be 2.x+)\")\n",
        "else:\n",
        "    print(\"[INFO] PyTorch 2.x installed, you'll be able to use the new features.\")"
      ],
      "metadata": {
        "id": "O5J5FU2ZS3BQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Get GPU info\n",
        "\n",
        "Why get GPU info?\n",
        "\n",
        "Because PyTorch 2.0 features (`torch.compile()`) works best on newer NVDIA GPUs\n",
        "\n",
        "Well, what's a newer NVDIA GPU?\n",
        "\n",
        "To find out if GPU is compatible see NVDIA GPU compatibility score - https://developer.nvidia.com/cuda-gpus\n",
        "If your GPU has a score of 8.0+, it can leverage *most* of if not *all* of the new PyTorch 2.0 features.\n",
        "\n",
        "GPUs under 8.0 can leverage PyTorch 2.0 however, the imporvements may not be noticable as those with 8.0+."
      ],
      "metadata": {
        "id": "K0qcyFDpS29Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure we're using a NVIDIA GPU\n",
        "if torch.cuda.is_available():\n",
        "  gpu_info = !nvidia-smi\n",
        "  gpu_info = '\\n'.join(gpu_info)\n",
        "  if gpu_info.find(\"failed\") >= 0:\n",
        "    print(\"Not connected to a GPU, to leverage the best of PyTorch 2.0, you should connect to a GPU.\")\n",
        "\n",
        "  # Get GPU name\n",
        "  gpu_name = !nvidia-smi --query-gpu=gpu_name --format=csv\n",
        "  gpu_name = gpu_name[1]\n",
        "  GPU_NAME = gpu_name.replace(\" \", \"_\") # remove underscores for easier saving\n",
        "  print(f'GPU name: {GPU_NAME}')\n",
        "\n",
        "  # Get GPU capability score\n",
        "  GPU_SCORE = torch.cuda.get_device_capability()\n",
        "  print(f\"GPU capability score: {GPU_SCORE}\")\n",
        "  if GPU_SCORE >= (8, 0):\n",
        "    print(f\"GPU score higher than or equal to (8, 0), PyTorch 2.x speedup features available.\")\n",
        "  else:\n",
        "    print(f\"GPU score lower than (8, 0), PyTorch 2.x speedup features will be limited (PyTorch 2.x speedups happen most on newer GPUs).\")\n",
        "\n",
        "  # Print GPU info\n",
        "  print(f\"GPU information:\\n{gpu_info}\")\n",
        "\n",
        "else:\n",
        "  print(\"PyTorch couldn't find a GPU, to leverage the best of PyTorch 2.0, you should connect to a GPU.\")"
      ],
      "metadata": {
        "id": "JOTH0lsPS63k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.1 Globally set devices\n",
        "\n",
        "Previously we'd have to set our tensors/models with `to(device)`\n",
        "\n",
        "* `tensor.to(device)`\n",
        "* `model.to(device)`\n",
        "\n",
        "But in PyTorch 2.0 it's possible to set the device with a context manager as well as a global device- https://pytorch.org/blog/pytorch-2.0-release/#beta-torchset_default_device-and-torchdevice-as-context-manager\n",
        "\n",
        "See docs:-https://pytorch.org/tutorials/recipes/recipes/changing_default_device.html"
      ],
      "metadata": {
        "id": "w3h-nQcXS6zI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Set the device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Set the device with context manager (requires PyTorch 2.x+)\n",
        "with torch.device(device):\n",
        "    # All tensors created in this block will be on device\n",
        "    layer = torch.nn.Linear(20, 30)\n",
        "    print(f\"Layer weights are on device: {layer.weight.device}\")\n",
        "    print(f\"Layer creating data on device: {layer(torch.randn(128, 20)).device}\")"
      ],
      "metadata": {
        "id": "q4AsGEz-S6u4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Set the device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Set the device globally requires pytorch2.x+\n",
        "torch.set_default_device(device)\n",
        "\n",
        "# All tensors created will be on the global device by default\n",
        "layer = torch.nn.Linear(20, 30)\n",
        "print(f\"Layer weights are on device: {layer.weight.device}\")\n",
        "print(f\"Layer creating data on device: {layer(torch.randn(128, 20)).device}\")"
      ],
      "metadata": {
        "id": "KoXZ9Jw6S6qT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Set the device globally\n",
        "torch.set_default_device(\"cpu\")\n",
        "\n",
        "# All tensors created will be on \"cpu\"\n",
        "layer = torch.nn.Linear(20, 30)\n",
        "print(f\"Layer weights are on device: {layer.weight.device}\")\n",
        "print(f\"Layer creating data on device: {layer(torch.randn(128, 20)).device}\")"
      ],
      "metadata": {
        "id": "CbuhqzPnS6mo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Setting up the experiments\n",
        "\n",
        "Time to test the speed\n",
        "\n",
        "To keep things simple we'll run 4 experiments\n",
        "\n",
        "* Model: ResNet50 from torchvision\n",
        "* Data:CIFAR10 from torchvision\n",
        "* Epochs: 5 (single run) 3x5(multi run)\n",
        "* Batch size: 128 (note: you may want to change this depending on the amount of memory your GPU has, this focusses on an A100)\n",
        "* Image size: 224 (note you may want to adjust this given the amount of GPU memory you have)"
      ],
      "metadata": {
        "id": "nJYzuc-1CGPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"TorchVision version: {torchvision.__version__}\")\n",
        "\n",
        "# Set the target device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "fe3ZXThLCGHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Create model and transforms"
      ],
      "metadata": {
        "id": "b9Ect7qWDtBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create model weights and transforms\n",
        "model_weights = torchvision.models.ResNet50_Weights.IMAGENET1K_V2 # <- use the latest weights (could also use .DEFAULT)\n",
        "transforms = model_weights.transforms()\n",
        "\n",
        "# Setup model\n",
        "model = torchvision.models.resnet50(weights=model_weights)\n",
        "\n",
        "# Count the number of parameters in the model\n",
        "total_params = sum(\n",
        "    param.numel() for param in model.parameters() # <- all params\n",
        "\t# param.numel() for param in model.parameters() if param.requires_grad # <- only trainable params\n",
        ")\n",
        "\n",
        "print(f\"Total parameters of model: {total_params} (the more parameters, the more GPU memory the model will use, the more *relative* of a speedup you'll get)\")\n",
        "print(f\"Model transforms:\\n{transforms}\")"
      ],
      "metadata": {
        "id": "HShc_RkVDs-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Note** PyTorch 2.0 *relative* speedups will be most noticeable when as much of the GPU as possible is being used. This means a larger model (more trainable parameters) may take longer to train on the whole but will relatively faster. E.g. a model with 1M params may take ~10 mins to train but a model with 25M parms may take ~20mins to train."
      ],
      "metadata": {
        "id": "uM3g7lVJHU3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(num_classes=10):\n",
        "  \"\"\"\n",
        "  Creates a ResNet50 model with the latest weights and transforms via torchvision.\n",
        "  \"\"\"\n",
        "  model_weights = torchvision.models.ResNet50_Weights.IMAGENET1K_V2\n",
        "  transforms = model_weights.transforms()\n",
        "  model = torchvision.models.resnet50(weights=model_weights)\n",
        "\n",
        "  # Adjust the number of output features in model to match the number of classes in the dataset\n",
        "  model.fc = torch.nn.Linear(in_features=2048,\n",
        "                             out_features=num_classes)\n",
        "  return model, transforms\n",
        "\n",
        "model, transforms = create_model()"
      ],
      "metadata": {
        "id": "ZAwZ05vmDs6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Speedups are most noticeable when a large portion of the GPU is being used\n",
        "Since modern GPUs are so fast at performing operations, you will often notice the majority of relative speedups when as much data as possible is on the GPU.\n",
        "\n",
        "This can be achieved by:\n",
        "\n",
        "* **Increasing the batch size** - More samples per batch means more samples on the GPU, for example, using a batch size of 256 instead of 32.\n",
        "* **Increasing data size** - For example, using larger image size, 224x224 instead of 32x32. A larger data size means that more tensor operations will be happening on the GPU.\n",
        "* **Increasing model size** - For example, using a larger model such as ResNet101 instead of ResNet50. A larger model means that more tensor operations will be happening on the GPU.\n",
        "* **Decreasing data transfer** - For example, setting up all your tensors to be on GPU memory, this minizes the amount of data transfer between the CPU and GPU."
      ],
      "metadata": {
        "id": "ZS-kwQcnE0Ff"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2.3 Checking available GPU memory limits"
      ],
      "metadata": {
        "id": "Kapc-aSmEz8C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#check  available GPU mem and total GPU mem\n",
        "total_free_gpu_memory, total_gpu_memory = torch.cuda.mem_get_info()\n",
        "print(f\"Total GPU memory: {round(total_gpu_memory*1e-9,3)} GB\")\n",
        "print(f\"Free GPU memory: {round(total_free_gpu_memory*1e-9,3)} GB\")"
      ],
      "metadata": {
        "id": "QNow5lijEzuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* If GPU has 16+ of free memory set batch size to 128\n",
        "* If GPU has less than 16GB free, set batch size to 32"
      ],
      "metadata": {
        "id": "YEjrKqT4KiM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set batch size depending on amount of GPU memory\n",
        "total_free_gpu_memory_gb = round(total_free_gpu_memory * 1e-9, 3)\n",
        "if total_free_gpu_memory_gb >= 16:\n",
        "  BATCH_SIZE = 128 # Note: you could experiment with higher values here if you like.\n",
        "  IMAGE_SIZE = 224\n",
        "  print(f\"GPU memory available is {total_free_gpu_memory_gb} GB, using batch size of {BATCH_SIZE} and image size {IMAGE_SIZE}\")\n",
        "else:\n",
        "  BATCH_SIZE = 32\n",
        "  IMAGE_SIZE = 128\n",
        "  print(f\"GPU memory available is {total_free_gpu_memory_gb} GB, using batch size of {BATCH_SIZE} and image size {IMAGE_SIZE}\")"
      ],
      "metadata": {
        "id": "UexsRBX4Ezaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transforms.crop_size = IMAGE_SIZE\n",
        "transforms.resize_size = IMAGE_SIZE\n",
        "print(f\"Updated data transforms:\\n{transforms}\")"
      ],
      "metadata": {
        "id": "Efo3sUunKvzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4 More potential speedups with TF32\n",
        "TF32 stands for TensorFloat-32, a data format which is a combination of 16-bit and 32-bit floating point numbers.\n",
        "\n",
        "TensorFloat32= a datatype from NVDIA that bridges gap between Float32 and Float16.\n",
        "\n",
        "Float32=a number is represented by 32 bytes (e.g 010101010110011=7)\n",
        "Float16= a number is represented by 16 bytes (e.g 01010101=4)\n",
        "\n",
        "You can read more about how it works on:- https://blogs.nvidia.com/blog/tensorfloat-32-precision-format/\n",
        "\n",
        "What we want is:\n",
        "1. Fast model training\n",
        "2. Accurate model training\n",
        "\n",
        "TF32 is available on Ampere GPUs+"
      ],
      "metadata": {
        "id": "xBVCjUmXKvwn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if GPU_SCORE >= (8, 0):\n",
        "  print(f\"[INFO] Using GPU with score: {GPU_SCORE}, enabling TensorFloat32 (TF32) computing (faster on new GPUs)\")\n",
        "  torch.backends.cuda.matmul.allow_tf32 = True\n",
        "else:\n",
        "  print(f\"[INFO] Using GPU with score: {GPU_SCORE}, TensorFloat32 (TF32) not available, to use it you need a GPU with score >= (8, 0)\")\n",
        "  torch.backends.cuda.matmul.allow_tf32 = False"
      ],
      "metadata": {
        "id": "YP3vpyyNKvra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.5 Preparing datasets\n",
        "\n",
        "As before, we discussed we're going to use CIFAR10.\n",
        "\n",
        "Homepage:-https://www.cs.toronto.edu/~kriz/cifar.html\n",
        "\n",
        "Can download dataset from torchvision"
      ],
      "metadata": {
        "id": "mTiQMpwYKvnU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xb4soYu0ADi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9yzPU4q_Y2vF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create train and test datasets\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "IMAGE_SIZE = 128\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='.',\n",
        "                                             train=True,\n",
        "                                             download=True,\n",
        "                                             transform=transforms)\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='.',\n",
        "                                            train=False, # want the test split\n",
        "                                            download=True,\n",
        "                                            transform=transforms)\n",
        "\n",
        "# Get the lengths of the datasets\n",
        "train_len = len(train_dataset)\n",
        "test_len = len(test_dataset)\n",
        "\n",
        "print(f\"[INFO] Train dataset length: {train_len}\")\n",
        "print(f\"[INFO] Test dataset length: {test_len}\")"
      ],
      "metadata": {
        "id": "zCf05rF5Kvg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2.6 Creatind DataLoaders"
      ],
      "metadata": {
        "id": "W6bxt6GeKvdg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Create DataLoaders\n",
        "import os\n",
        "NUM_WORKERS = os.cpu_count() # <- use all available CPU cores (this number can be tweaked through experimentation but generally more workers means faster dataloading from CPU to GPU)\n",
        "\n",
        "train_dataloader = DataLoader(dataset=train_dataset,\n",
        "                              batch_size=BATCH_SIZE,\n",
        "                              shuffle=True,\n",
        "                              num_workers=NUM_WORKERS)\n",
        "\n",
        "test_dataloader = DataLoader(dataset=test_dataset,\n",
        "                              batch_size=BATCH_SIZE,\n",
        "                              shuffle=False,\n",
        "                              num_workers=NUM_WORKERS)\n",
        "\n",
        "# Print details\n",
        "print(f\"Train dataloader length: {len(train_dataloader)} batches of size {BATCH_SIZE}\")\n",
        "print(f\"Test dataloader length: {len(test_dataloader)} batches of size {BATCH_SIZE}\")\n",
        "print(f\"Using number of workers: {NUM_WORKERS} (generally more workers means faster dataloading from CPU to GPU)\")"
      ],
      "metadata": {
        "id": "7_vvM250KvSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.7 Create training and testing loops\n",
        "\n",
        "Want to create:\n",
        "* A training and test loop + a timing step for each, so we know how long our model take to train/test"
      ],
      "metadata": {
        "id": "GnsHHmzbY5g2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from tqdm.auto import tqdm\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "def train_step(epoch: int,\n",
        "               model: torch.nn.Module,\n",
        "               dataloader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               optimizer: torch.optim.Optimizer,\n",
        "               device: torch.device,\n",
        "               disable_progress_bar: bool = False) -> Tuple[float, float]:\n",
        "  \"\"\"Trains a PyTorch model for a single epoch.\n",
        "\n",
        "  Turns a target PyTorch model to training mode and then\n",
        "  runs through all of the required training steps (forward\n",
        "  pass, loss calculation, optimizer step).\n",
        "\n",
        "  Args:\n",
        "    model: A PyTorch model to be trained.\n",
        "    dataloader: A DataLoader instance for the model to be trained on.\n",
        "    loss_fn: A PyTorch loss function to minimize.\n",
        "    optimizer: A PyTorch optimizer to help minimize the loss function.\n",
        "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
        "\n",
        "  Returns:\n",
        "    A tuple of training loss and training accuracy metrics.\n",
        "    In the form (train_loss, train_accuracy). For example:\n",
        "\n",
        "    (0.1112, 0.8743)\n",
        "  \"\"\"\n",
        "  # Put model in train mode\n",
        "  model.train()\n",
        "\n",
        "  # Setup train loss and train accuracy values\n",
        "  train_loss, train_acc = 0, 0\n",
        "\n",
        "  # Loop through data loader data batches\n",
        "  progress_bar = tqdm(\n",
        "        enumerate(dataloader),\n",
        "        desc=f\"Training Epoch {epoch}\",\n",
        "        total=len(dataloader),\n",
        "        disable=disable_progress_bar\n",
        "    )\n",
        "\n",
        "  for batch, (X, y) in progress_bar:\n",
        "      # Send data to target device\n",
        "      X, y = X.to(device), y.to(device)\n",
        "\n",
        "      # 1. Forward pass\n",
        "      y_pred = model(X)\n",
        "\n",
        "      # 2. Calculate  and accumulate loss\n",
        "      loss = loss_fn(y_pred, y)\n",
        "      train_loss += loss.item()\n",
        "\n",
        "      # 3. Optimizer zero grad\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # 4. Loss backward\n",
        "      loss.backward()\n",
        "\n",
        "      # 5. Optimizer step\n",
        "      optimizer.step()\n",
        "\n",
        "      # Calculate and accumulate accuracy metric across all batches\n",
        "      y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
        "      train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n",
        "\n",
        "      # Update progress bar\n",
        "      progress_bar.set_postfix(\n",
        "            {\n",
        "                \"train_loss\": train_loss / (batch + 1),\n",
        "                \"train_acc\": train_acc / (batch + 1),\n",
        "            }\n",
        "        )\n",
        "\n",
        "\n",
        "  # Adjust metrics to get average loss and accuracy per batch\n",
        "  train_loss = train_loss / len(dataloader)\n",
        "  train_acc = train_acc / len(dataloader)\n",
        "  return train_loss, train_acc\n",
        "\n",
        "def test_step(epoch: int,\n",
        "              model: torch.nn.Module,\n",
        "              dataloader: torch.utils.data.DataLoader,\n",
        "              loss_fn: torch.nn.Module,\n",
        "              device: torch.device,\n",
        "              disable_progress_bar: bool = False) -> Tuple[float, float]:\n",
        "  \"\"\"Tests a PyTorch model for a single epoch.\n",
        "\n",
        "  Turns a target PyTorch model to \"eval\" mode and then performs\n",
        "  a forward pass on a testing dataset.\n",
        "\n",
        "  Args:\n",
        "    model: A PyTorch model to be tested.\n",
        "    dataloader: A DataLoader instance for the model to be tested on.\n",
        "    loss_fn: A PyTorch loss function to calculate loss on the test data.\n",
        "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
        "\n",
        "  Returns:\n",
        "    A tuple of testing loss and testing accuracy metrics.\n",
        "    In the form (test_loss, test_accuracy). For example:\n",
        "\n",
        "    (0.0223, 0.8985)\n",
        "  \"\"\"\n",
        "  # Put model in eval mode\n",
        "  model.eval()\n",
        "\n",
        "  # Setup test loss and test accuracy values\n",
        "  test_loss, test_acc = 0, 0\n",
        "\n",
        "  # Loop through data loader data batches\n",
        "  progress_bar = tqdm(\n",
        "      enumerate(dataloader),\n",
        "      desc=f\"Testing Epoch {epoch}\",\n",
        "      total=len(dataloader),\n",
        "      disable=disable_progress_bar\n",
        "  )\n",
        "\n",
        "  # Turn on inference context manager\n",
        "  with torch.no_grad(): # no_grad() required for PyTorch 2.0, I found some errors with `torch.inference_mode()`, please let me know if this is not the case\n",
        "      # Loop through DataLoader batches\n",
        "      for batch, (X, y) in progress_bar:\n",
        "          # Send data to target device\n",
        "          X, y = X.to(device), y.to(device)\n",
        "\n",
        "          # 1. Forward pass\n",
        "          test_pred_logits = model(X)\n",
        "\n",
        "          # 2. Calculate and accumulate loss\n",
        "          loss = loss_fn(test_pred_logits, y)\n",
        "          test_loss += loss.item()\n",
        "\n",
        "          # Calculate and accumulate accuracy\n",
        "          test_pred_labels = test_pred_logits.argmax(dim=1)\n",
        "          test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n",
        "\n",
        "          # Update progress bar\n",
        "          progress_bar.set_postfix(\n",
        "              {\n",
        "                  \"test_loss\": test_loss / (batch + 1),\n",
        "                  \"test_acc\": test_acc / (batch + 1),\n",
        "              }\n",
        "          )\n",
        "\n",
        "  # Adjust metrics to get average loss and accuracy per batch\n",
        "  test_loss = test_loss / len(dataloader)\n",
        "  test_acc = test_acc / len(dataloader)\n",
        "  return test_loss, test_acc\n",
        "\n",
        "def train(model: torch.nn.Module,\n",
        "          train_dataloader: torch.utils.data.DataLoader,\n",
        "          test_dataloader: torch.utils.data.DataLoader,\n",
        "          optimizer: torch.optim.Optimizer,\n",
        "          loss_fn: torch.nn.Module,\n",
        "          epochs: int,\n",
        "          device: torch.device,\n",
        "          disable_progress_bar: bool = False) -> Dict[str, List]:\n",
        "  \"\"\"Trains and tests a PyTorch model.\n",
        "\n",
        "  Passes a target PyTorch models through train_step() and test_step()\n",
        "  functions for a number of epochs, training and testing the model\n",
        "  in the same epoch loop.\n",
        "\n",
        "  Calculates, prints and stores evaluation metrics throughout.\n",
        "\n",
        "  Args:\n",
        "    model: A PyTorch model to be trained and tested.\n",
        "    train_dataloader: A DataLoader instance for the model to be trained on.\n",
        "    test_dataloader: A DataLoader instance for the model to be tested on.\n",
        "    optimizer: A PyTorch optimizer to help minimize the loss function.\n",
        "    loss_fn: A PyTorch loss function to calculate loss on both datasets.\n",
        "    epochs: An integer indicating how many epochs to train for.\n",
        "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
        "\n",
        "  Returns:\n",
        "    A dictionary of training and testing loss as well as training and\n",
        "    testing accuracy metrics. Each metric has a value in a list for\n",
        "    each epoch.\n",
        "    In the form: {train_loss: [...],\n",
        "                  train_acc: [...],\n",
        "                  test_loss: [...],\n",
        "                  test_acc: [...]}\n",
        "    For example if training for epochs=2:\n",
        "                 {train_loss: [2.0616, 1.0537],\n",
        "                  train_acc: [0.3945, 0.3945],\n",
        "                  test_loss: [1.2641, 1.5706],\n",
        "                  test_acc: [0.3400, 0.2973]}\n",
        "  \"\"\"\n",
        "  # Create empty results dictionary\n",
        "  results = {\"train_loss\": [],\n",
        "      \"train_acc\": [],\n",
        "      \"test_loss\": [],\n",
        "      \"test_acc\": [],\n",
        "      \"train_epoch_time\": [],\n",
        "      \"test_epoch_time\": []\n",
        "  }\n",
        "\n",
        "  # Loop through training and testing steps for a number of epochs\n",
        "  for epoch in tqdm(range(epochs), disable=disable_progress_bar):\n",
        "\n",
        "      # Perform training step and time it\n",
        "      train_epoch_start_time = time.time()\n",
        "      train_loss, train_acc = train_step(epoch=epoch,\n",
        "                                        model=model,\n",
        "                                        dataloader=train_dataloader,\n",
        "                                        loss_fn=loss_fn,\n",
        "                                        optimizer=optimizer,\n",
        "                                        device=device,\n",
        "                                        disable_progress_bar=disable_progress_bar)\n",
        "      train_epoch_end_time = time.time()\n",
        "      train_epoch_time = train_epoch_end_time - train_epoch_start_time\n",
        "\n",
        "      # Perform testing step and time it\n",
        "      test_epoch_start_time = time.time()\n",
        "      test_loss, test_acc = test_step(epoch=epoch,\n",
        "                                      model=model,\n",
        "                                      dataloader=test_dataloader,\n",
        "                                      loss_fn=loss_fn,\n",
        "                                      device=device,\n",
        "                                      disable_progress_bar=disable_progress_bar)\n",
        "      test_epoch_end_time = time.time()\n",
        "      test_epoch_time = test_epoch_end_time - test_epoch_start_time\n",
        "\n",
        "      # Print out what's happening\n",
        "      print(\n",
        "          f\"Epoch: {epoch+1} | \"\n",
        "          f\"train_loss: {train_loss:.4f} | \"\n",
        "          f\"train_acc: {train_acc:.4f} | \"\n",
        "          f\"test_loss: {test_loss:.4f} | \"\n",
        "          f\"test_acc: {test_acc:.4f} | \"\n",
        "          f\"train_epoch_time: {train_epoch_time:.4f} | \"\n",
        "          f\"test_epoch_time: {test_epoch_time:.4f}\"\n",
        "      )\n",
        "\n",
        "      # Update results dictionary\n",
        "      results[\"train_loss\"].append(train_loss)\n",
        "      results[\"train_acc\"].append(train_acc)\n",
        "      results[\"test_loss\"].append(test_loss)\n",
        "      results[\"test_acc\"].append(test_acc)\n",
        "      results[\"train_epoch_time\"].append(train_epoch_time)\n",
        "      results[\"test_epoch_time\"].append(test_epoch_time)\n",
        "\n",
        "  # Return the filled results at the end of the epochs\n",
        "  return results"
      ],
      "metadata": {
        "id": "H3nc4Zs4Y5U0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Time models across single run\n",
        "\n",
        "* Experiment 1=Single run without `torch.compile`\n",
        "* Experiment 2= Single run with `torch.compile`"
      ],
      "metadata": {
        "id": "R248whj3bK0h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Experiment 1 - Single run, no compile"
      ],
      "metadata": {
        "id": "jDLRrdm3bVOO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the number of epochs as a constant\n",
        "NUM_EPOCHS = 5\n",
        "\n",
        "# Set the learning rate as a constant (this can be changed to get better results but for now we're just focused on time)\n",
        "LEARNING_RATE = 0.003"
      ],
      "metadata": {
        "id": "uAtBwFXgbONa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note**: depending on the GPU/machine the following code may take a while to run. E.g in my experience on an A100, it takes about ~7 minutes."
      ],
      "metadata": {
        "id": "wCd6W0zbdiLe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create model\n",
        "model, transforms = create_model()\n",
        "model.to(device)\n",
        "\n",
        "# Create loss function and optimizer\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(),\n",
        "                             lr=LEARNING_RATE)\n",
        "\n",
        "# Train model and track results\n",
        "single_run_no_compile_results = train(model=model,\n",
        "                                      train_dataloader=train_dataloader,\n",
        "                                      test_dataloader=test_dataloader,\n",
        "                                      loss_fn=loss_fn,\n",
        "                                      optimizer=optimizer,\n",
        "                                      epochs=NUM_EPOCHS,\n",
        "                                      device=device)"
      ],
      "metadata": {
        "id": "QxIeQ9o2bPE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Experiment 2 - Single run, with compile\n",
        "\n",
        "Same as setup experiment 1 but now has noew line `torch.compile`."
      ],
      "metadata": {
        "id": "7Jdh_-_7cf4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create model and transforms\n",
        "model, transforms = create_model()\n",
        "model.to(device)\n",
        "\n",
        "# Create loss function and optimizer\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(),\n",
        "                             lr=LEARNING_RATE)\n",
        "\n",
        "# Compile the model and time how long it takes\n",
        "compile_start_time = time.time()\n",
        "\n",
        "### New in PyTorch 2.x ###\n",
        "compiled_model = torch.compile(model)\n",
        "##########################\n",
        "\n",
        "compile_end_time = time.time()\n",
        "compile_time = compile_end_time - compile_start_time\n",
        "print(f\"Time to compile: {compile_time} | Note: The first time you compile your model, the first few epochs will be slower than subsequent runs.\")\n",
        "\n",
        "# Train the compiled model\n",
        "single_run_compile_results = train(model=compiled_model,\n",
        "                                   train_dataloader=train_dataloader,\n",
        "                                   test_dataloader=test_dataloader,\n",
        "                                   loss_fn=loss_fn,\n",
        "                                   optimizer=optimizer,\n",
        "                                   epochs=NUM_EPOCHS,\n",
        "                                   device=device)"
      ],
      "metadata": {
        "id": "CUIiUGFWcfwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 Compare the results of experiment 1 and 2"
      ],
      "metadata": {
        "id": "8QD1QUaRcfm5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn experiment results into dataframes\n",
        "import pandas as pd\n",
        "single_run_no_compile_results_df = pd.DataFrame(single_run_no_compile_results)\n",
        "single_run_compile_results_df = pd.DataFrame(single_run_compile_results)"
      ],
      "metadata": {
        "id": "5mJIFpiDcuf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check out the head of one of the results dataframes\n",
        "single_run_no_compile_results_df.head()"
      ],
      "metadata": {
        "id": "x5--LfTccudR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create filename to save the results\n",
        "DATASET_NAME = \"CIFAR10\"\n",
        "MODEL_NAME = \"ResNet50\""
      ],
      "metadata": {
        "id": "ISathixmcuZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def plot_mean_epoch_times(non_compiled_results: pd.DataFrame,\n",
        "                          compiled_results: pd.DataFrame,\n",
        "                          multi_runs: bool=False,\n",
        "                          num_runs: int=0,\n",
        "                          save: bool=False,\n",
        "                          save_path: str=\"\",\n",
        "                          dataset_name: str=DATASET_NAME,\n",
        "                          model_name: str=MODEL_NAME,\n",
        "                          num_epochs: int=NUM_EPOCHS,\n",
        "                          image_size: int=IMAGE_SIZE,\n",
        "                          batch_size: int=BATCH_SIZE) -> plt.figure:\n",
        "\n",
        "    # Get the mean epoch times from the non-compiled models\n",
        "    mean_train_epoch_time = non_compiled_results.train_epoch_time.mean()\n",
        "    mean_test_epoch_time = non_compiled_results.test_epoch_time.mean()\n",
        "    mean_results = [mean_train_epoch_time, mean_test_epoch_time]\n",
        "\n",
        "    # Get the mean epoch times from the compiled models\n",
        "    mean_compile_train_epoch_time = compiled_results.train_epoch_time.mean()\n",
        "    mean_compile_test_epoch_time = compiled_results.test_epoch_time.mean()\n",
        "    mean_compile_results = [mean_compile_train_epoch_time, mean_compile_test_epoch_time]\n",
        "\n",
        "    # Calculate the percentage difference between the mean compile and non-compile train epoch times\n",
        "    train_epoch_time_diff = mean_compile_train_epoch_time - mean_train_epoch_time\n",
        "    train_epoch_time_diff_percent = (train_epoch_time_diff / mean_train_epoch_time) * 100\n",
        "\n",
        "    # Calculate the percentage difference between the mean compile and non-compile test epoch times\n",
        "    test_epoch_time_diff = mean_compile_test_epoch_time - mean_test_epoch_time\n",
        "    test_epoch_time_diff_percent = (test_epoch_time_diff / mean_test_epoch_time) * 100\n",
        "\n",
        "    # Print the mean difference percentages\n",
        "    print(f\"Mean train epoch time difference: {round(train_epoch_time_diff_percent, 3)}% (negative means faster)\")\n",
        "    print(f\"Mean test epoch time difference: {round(test_epoch_time_diff_percent, 3)}% (negative means faster)\")\n",
        "\n",
        "    # Create a bar plot of the mean train and test epoch time for both compiled and non-compiled models\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    width = 0.3\n",
        "    x_indicies = np.arange(len(mean_results))\n",
        "\n",
        "    plt.bar(x=x_indicies, height=mean_results, width=width, label=\"non_compiled_results\")\n",
        "    plt.bar(x=x_indicies + width, height=mean_compile_results, width=width, label=\"compiled_results\")\n",
        "    plt.xticks(x_indicies + width / 2, (\"Train Epoch\", \"Test Epoch\"))\n",
        "    plt.ylabel(\"Mean epoch time (seconds, lower is better)\")\n",
        "\n",
        "    # Create the title based on the parameters passed to the function\n",
        "    if multi_runs:\n",
        "        plt.suptitle(\"Multiple run results\")\n",
        "        plt.title(f\"GPU: {gpu_name} | Epochs: {num_epochs} ({num_runs} runs) | Data: {dataset_name} | Model: {model_name} | Image size: {image_size} | Batch size: {batch_size}\")\n",
        "    else:\n",
        "        plt.suptitle(\"Single run results\")\n",
        "        plt.title(f\"GPU: {gpu_name} | Epochs: {num_epochs} | Data: {dataset_name} | Model: {model_name} | Image size: {image_size} | Batch size: {batch_size}\")\n",
        "    plt.legend();\n",
        "\n",
        "    # Save the figure\n",
        "    if save:\n",
        "        assert save_path != \"\", \"Please specify a save path to save the model figure to via the save_path parameter.\"\n",
        "        plt.savefig(save_path)\n",
        "        print(f\"[INFO] Plot saved to {save_path}\")"
      ],
      "metadata": {
        "id": "L1EiRJmHcuWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create directory for saving figures\n",
        "import os\n",
        "dir_to_save_figures_in = \"pytorch_2_results/figures/\"\n",
        "os.makedirs(dir_to_save_figures_in, exist_ok=True)\n",
        "\n",
        "# Create a save path for the single run results\n",
        "save_path_multi_run = f\"{dir_to_save_figures_in}single_run_{GPU_NAME}_{MODEL_NAME}_{DATASET_NAME}_{IMAGE_SIZE}_train_epoch_time.png\"\n",
        "print(f\"[INFO] Save path for single run results: {save_path_multi_run}\")\n",
        "\n",
        "# Plot the results and save the figures\n",
        "plot_mean_epoch_times(non_compiled_results=single_run_no_compile_results_df,\n",
        "                      compiled_results=single_run_compile_results_df,\n",
        "                      multi_runs=False,\n",
        "                      save_path=save_path_multi_run,\n",
        "                      save=True)"
      ],
      "metadata": {
        "id": "AAz3MD1ac7O8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4 Save single run results to file with GPU details"
      ],
      "metadata": {
        "id": "icpVqFjgc7Me"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a directory for single_run results\n",
        "import os\n",
        "pytorch_2_results_dir = \"pytorch_2_results\"\n",
        "pytorch_2_single_run_results_dir = f\"{pytorch_2_results_dir}/single_run_results\"\n",
        "os.makedirs(pytorch_2_single_run_results_dir, exist_ok=True)\n",
        "\n",
        "# Create filenames for each of the dataframes\n",
        "save_name_for_non_compiled_results = f\"single_run_non_compiled_results_{DATASET_NAME}_{MODEL_NAME}_{GPU_NAME}.csv\"\n",
        "save_name_for_compiled_results = f\"single_run_compiled_results_{DATASET_NAME}_{MODEL_NAME}_{GPU_NAME}.csv\"\n",
        "\n",
        "# Create filepaths to save the results to\n",
        "single_run_no_compile_save_path = f\"{pytorch_2_single_run_results_dir}/{save_name_for_non_compiled_results}\"\n",
        "single_run_compile_save_path = f\"{pytorch_2_single_run_results_dir}/{save_name_for_compiled_results}\"\n",
        "print(f\"[INFO] Saving non-compiled experiment 1 results to: {single_run_no_compile_save_path}\")\n",
        "print(f\"[INFO] Saving compiled experiment 2 results to: {single_run_compile_save_path}\")\n",
        "\n",
        "# Save the results\n",
        "single_run_no_compile_results_df.to_csv(single_run_no_compile_save_path)\n",
        "single_run_compile_results_df.to_csv(single_run_compile_save_path)"
      ],
      "metadata": {
        "id": "5rVOBlywc7Jm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Time models across multiple runs\n",
        "\n",
        "Time for multi-run experiments\n",
        "\n",
        "* Experiment 3=3x5 epochs without `torch.compile()`\n",
        "* Experiment 4=3x5 epochs with `torch.compile()`\n",
        "\n",
        "Before running experiment 3 and 4, let's create 3 functions:\n",
        "\n",
        "1. **Experiment 3:** `create_and_train_non_compiled_model()`-creates and trains a model (can put this function in a loop for multiple runs)\n",
        "2. **Experiment 4:** `createcompiled_model()`-creates and compiled model, returns the compiled model\n",
        "3. **Experiment 4:** `train_compiled_model()`-trains a compiled model for a single run (can put this in a loop to train for multiple runs )"
      ],
      "metadata": {
        "id": "6G3MclOTc7Gi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_and_train_non_compiled_model(epochs: int=NUM_EPOCHS,\n",
        "                                        learning_rate: float=LEARNING_RATE,\n",
        "                                        disable_progress_bar: bool=False):\n",
        "\n",
        "  \"\"\"\n",
        "  Create and train a non-compiled PyTorch model.\n",
        "  \"\"\"\n",
        "  model,_=create_model()\n",
        "  model.to(device)\n",
        "\n",
        "  loss_fn=torch.nn.CrossEntropyLoss()\n",
        "  optimizer=torch.optim.Adam(model.parameters(),\n",
        "                             lr=learning_rate)\n",
        "\n",
        "  results =train(model=model,\n",
        "                 train_dataloader=train_dataloader,\n",
        "                 test_dataloader=test_dataloader,\n",
        "                 loss_fn=loss_fn,\n",
        "                 optimizer=optimizer,\n",
        "                 epochs=epochs,\n",
        "                 device=device,\n",
        "                 disable_progress_bar=disable_progress_bar)\n",
        "\n",
        "  return results\n",
        "\n",
        "\n",
        "def create_compiled_model():\n",
        "  \"\"\"\n",
        "  Create a compiled PyTorch model and return it,\n",
        "  \"\"\"\n",
        "  model,_=create_model()\n",
        "  model.to(device)\n",
        "\n",
        "  compile_start_time = time.time()\n",
        "\n",
        "  ###new to PyTorch 2.0111##\n",
        "  compiled_model = torch.compile(model)\n",
        "\n",
        "  compile_end_time = time.time()\n",
        "  compile_time = compile_end_time - compile_start_time\n",
        "\n",
        "  print(f\"Time to compile: {compile_time} | Note: The first time you compile your model, the first few epochs will be slower than subsequent runs.\")\n",
        "\n",
        "  return compiled_model\n",
        "\n",
        "def train_compiled_model(model=compiled_model,\n",
        "                         epochs: int=NUM_EPOCHS,\n",
        "                          learning_rate: float=LEARNING_RATE,\n",
        "                          disable_progress_bar: bool=False):\n",
        "  \"\"\"\n",
        "  Train a compiled model and return the results.\n",
        "  \"\"\"\n",
        "  loss_fn=torch.nn.CrossEntropyLoss()\n",
        "  optimizer=torch.optim.Adam(model.parameters(),\n",
        "                             lr=learning_rate)\n",
        "\n",
        "  compiled_results = train(model=model,\n",
        "                         train_dataloader=train_dataloader,\n",
        "                         test_dataloader=test_dataloader,\n",
        "                         loss_fn=loss_fn,\n",
        "                         optimizer=optimizer,\n",
        "                         epochs=epochs,\n",
        "                         device=device,\n",
        "                         disable_progress_bar=disable_progress_bar)\n",
        "\n",
        "  return compiled_results"
      ],
      "metadata": {
        "id": "t0ZF8gjTc7Df"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Experiment 3 - Multiple runs, no compile\n",
        "\n",
        "**Note:** Because of multiple runs the code below may take a while yo run. If one single run takes ~7mins on a A100 the following may take about ~20 mins.\n",
        "\n",
        "> One of the most annoying things about ML is that models take a while to train and one of the ebst things is that models take a while to train so plenty of time to think about life and stuff."
      ],
      "metadata": {
        "id": "8k_gizJKc7AJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run non-compiled model for multiple runs\n",
        "NUM_RUNS=3\n",
        "NUM_EPOCHS = 5\n",
        "\n",
        "#create an empty list to store run results\n",
        "non_compile_results_multiple_runs=[]\n",
        "\n",
        "for i in tqdm(range(NUM_RUNS)):\n",
        "  print(f\"[INFO] run {i+1} of {NUM_RUNS} for no compiled model\")\n",
        "  results=create_and_train_non_compiled_model(epochs=NUM_EPOCHS,disable_progress_bar=True)\n",
        "  non_compile_results_multiple_runs.append(results)"
      ],
      "metadata": {
        "id": "ZjQ060_Uc69K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#go through the non_compile_results_multiple_runs and create a DataFrame\n",
        "non_compile_results_dfs=[]\n",
        "for result in non_compile_results_multiple_runs:\n",
        "    results_df = pd.DataFrame(result)\n",
        "    non_compile_results_dfs.append(results_df)\n",
        "non_compile_results_multiple_runs_df=pd.concat(non_compile_results_dfs)\n",
        "\n",
        "# Get the averages across the multiple runs\n",
        "non_compile_results_multiple_runs_df = non_compile_results_multiple_runs_df.groupby(non_compile_results_multiple_runs_df.index).mean()\n",
        "non_compile_results_multiple_runs_df\n"
      ],
      "metadata": {
        "id": "iJPU4pvsbkuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4.2 Experiment 4 - Multiple runs, with compile"
      ],
      "metadata": {
        "id": "hqgbO1bYbkrs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "compiled_model = create_compiled_model()\n",
        "\n",
        "#Creat empty list for compiled results\n",
        "compiled_results_multiple_runs=[]\n",
        "\n",
        "#run compiled model for multiple runs\n",
        "for i in tqdm(range(NUM_RUNS)):\n",
        "  print(f\"[INFO] Run {i+1} of {NUM_RUNS}\")\n",
        "  #train the compiled model note: the model will only be compiled once and then re-used for subsequent runs\n",
        "  results=train_compiled_model(model=compiled_model, epochs=NUM_EPOCHS,disable_progress_bar=True)\n",
        "  compiled_results_multiple_runs.append(results)"
      ],
      "metadata": {
        "id": "J8Rpm9LdlRm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Go through compile_results_multiple_runs and create a dataframe for each run then concatenate them together\n",
        "compile_results_dfs = []\n",
        "for result in compiled_results_multiple_runs:\n",
        "    result_df = pd.DataFrame(result)\n",
        "    compile_results_dfs.append(result_df)\n",
        "compile_results_multiple_runs_df = pd.concat(compile_results_dfs)\n",
        "\n",
        "# Get the averages across the multiple runs\n",
        "compile_results_multiple_runs_df = compile_results_multiple_runs_df.groupby(compile_results_multiple_runs_df.index).mean() # .index = groupby the epoch number\n",
        "compile_results_multiple_runs_df"
      ],
      "metadata": {
        "id": "9c7dbx9RlRfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4.3 Compare results of experiment 3 and 4"
      ],
      "metadata": {
        "id": "-AqNxXFtqaG7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a directory to save the multi-run figure to\n",
        "os.makedirs(\"pytorch_2_results/figures\", exist_ok=True)\n",
        "\n",
        "# Create a path to save the figure for multiple runs\n",
        "save_path_multi_run = f\"pytorch_2_results/figures/multi_run_{GPU_NAME}_{MODEL_NAME}_{DATASET_NAME}_{IMAGE_SIZE}_train_epoch_time.png\"\n",
        "\n",
        "# Plot the mean epoch times for experiment 3 and 4\n",
        "plot_mean_epoch_times(non_compiled_results=non_compile_results_multiple_runs_df,\n",
        "                      compiled_results=compile_results_multiple_runs_df,\n",
        "                      multi_runs=True,\n",
        "                      num_runs=NUM_RUNS,\n",
        "                      save_path=save_path_multi_run,\n",
        "                      save=True)"
      ],
      "metadata": {
        "id": "IJ6yx9HoqaA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4.4 Save multi run results to file with GPU details"
      ],
      "metadata": {
        "id": "do4-bh_EqZ7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a directory for multi_run results\n",
        "import os\n",
        "pytorch_2_results_dir = \"pytorch_2_results\"\n",
        "pytorch_2_multi_run_results_dir = f\"{pytorch_2_results_dir}/multi_run_results\"\n",
        "os.makedirs(pytorch_2_multi_run_results_dir, exist_ok=True)\n",
        "\n",
        "# Create filenames for each of the dataframes\n",
        "save_name_for_multi_run_non_compiled_results = f\"multi_run_non_compiled_results_{NUM_RUNS}_runs_{DATASET_NAME}_{MODEL_NAME}_{GPU_NAME}.csv\"\n",
        "save_name_for_multi_run_compiled_results = f\"multi_run_compiled_results_{NUM_RUNS}_runs_{DATASET_NAME}_{MODEL_NAME}_{GPU_NAME}.csv\"\n",
        "\n",
        "# Create filepaths to save the results to\n",
        "multi_run_no_compile_save_path = f\"{pytorch_2_multi_run_results_dir}/{save_name_for_non_compiled_results}\"\n",
        "multi_run_compile_save_path = f\"{pytorch_2_multi_run_results_dir}/{save_name_for_compiled_results}\"\n",
        "print(f\"[INFO] Saving experiment 3 non-compiled results to: {multi_run_no_compile_save_path}\")\n",
        "print(f\"[INFO] Saving experiment 4 compiled results to: {multi_run_compile_save_path}\")\n",
        "\n",
        "# Save the results\n",
        "non_compile_results_multiple_runs_df.to_csv(multi_run_no_compile_save_path)\n",
        "compile_results_multiple_runs_df.to_csv(multi_run_compile_save_path)"
      ],
      "metadata": {
        "id": "rtzbcAOzqZ11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Possible improvements and extensions\n",
        "We've explored the fundamentals of `torch.compile()` and wrote code for several experiments to test how it performs.\n",
        "\n",
        "But there's still more we could do.\n",
        "\n",
        "As we've discussed, many of the speedups in PyTorch 2.0 and `torch.compile()` come from using newer GPUs (e.g. A100 and above) and using as much of the GPU as possible (larger batch sizes, larger model sizes).\n",
        "\n",
        "For even more speedups, I'd recommend researching/trying the following:\n",
        "\n",
        "* **More powerful CPUs** - I have a sneaking suspicion that Google Colab instances are limited to 2 CPU cores, speedup numbers could be improved with more CPUs. This could be tracked via the PyTorch Profiler(https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html) (a tool to find what processes take what time).\n",
        "* **Using mixed precision training** - newer GPUs have the ability to handle difference precision types (e.g. `torch.float16` and `torch.bfloat16`) which enable faster training and inference. I'd suspect you'll see an even larger speedup than we've seen here by using mixed precision training. For more on this, see the PyTorch documentation (https://pytorch.org/docs/stable/notes/amp_examples.html#amp-examples) for automatic mixed precision (also called AMP) with PyTorch.\n",
        "* **Transformer based models may see more relative speedups than convolutional models** - PyTorch 2.0 includes a stable release (https://pytorch.org/blog/pytorch-2.0-release/#stable-accelerated-pytorch-2-transformers) for accelerated transformer models (models which use the attention mechanism). The main speedups come from an improved implementation of `scaled_dot_product_attention()` which automatically selects the best version of attention to use based on the hardware you're computing on. You can see more in the dedicated PyTorch tutorial. (https://pytorch.org/tutorials/intermediate/scaled_dot_product_attention_tutorial.html)\n",
        "* **Train for longer** - As previously discussed, the speedups from `torch.compile()` are likely to be more noticeable when training for longer. A great exercise would be to train over a longer number of epochs, potentially on a different dataset with a different model (e.g. a transformer) and see how the speedups compare."
      ],
      "metadata": {
        "id": "BHQL8ZTVqmhL"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SZB6KwIrqmd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3rMwPfvQqmau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2m0wgKbdqmWY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}